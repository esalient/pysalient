{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5d9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from pysalient import io as io\n",
    "from pysalient import visualisation as vis\n",
    "from pysalient.evaluation import evaluation\n",
    "from pysalient.evaluation import compare_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67955af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 21356\n",
      "['encounter_id', 'event_timestamp', 'culture_event', 'suspected_infection', 'true_label', 'prediction_proba_1', 'prediction_proba_2']\n",
      "Number of true labels (1): 789\n",
      "  encounter_id  event_timestamp  culture_event  suspected_infection  \\\n",
      "0     0666505c              2.0            1.0                  1.0   \n",
      "1     0666505c              2.0            1.0                  1.0   \n",
      "2     0666505c              3.0            1.0                  1.0   \n",
      "3     0666505c              3.0            1.0                  1.0   \n",
      "4     0666505c              4.0            1.0                  1.0   \n",
      "\n",
      "   true_label  prediction_proba_1  prediction_proba_2  \n",
      "0           1            0.185334            0.531925  \n",
      "1           1            0.185334            0.531925  \n",
      "2           1            0.134316            0.360046  \n",
      "3           1            0.134316            0.360046  \n",
      "4           1            0.118005            0.167655  \n",
      "Number of unique encounter groups: 100\n",
      "Number of encounter groups with at least one true positive: 50\n"
     ]
    }
   ],
   "source": [
    "sample_data_path = os.path.join(\"data\", \"anonymised_sample.parquet\")\n",
    "# count rows\n",
    "table = pq.read_table(sample_data_path)\n",
    "print(f\"Number of rows: {table.num_rows}\")\n",
    "# print column names\n",
    "print(table.column_names)\n",
    "# Convert the 'true_label' column to a pandas Series\n",
    "true_label_series = table[\"true_label\"].to_pandas()\n",
    "\n",
    "# Count the number of true labels (1)\n",
    "true_count = (true_label_series == 1).sum()\n",
    "\n",
    "print(f\"Number of true labels (1): {true_count}\")\n",
    "\n",
    "# Convert the table to a pandas DataFrame for easier grouping\n",
    "df = table.to_pandas()\n",
    "\n",
    "# show table\n",
    "print(df.head(5))\n",
    "\n",
    "\n",
    "grouped = df.groupby(\"encounter_id\")\n",
    "\n",
    "# Count the number of unique groups (encounters)\n",
    "num_groups = df[\"encounter_id\"].nunique()\n",
    "print(f\"Number of unique encounter groups: {num_groups}\")\n",
    "\n",
    "# Calculate the sum of 'true_label' for each group\n",
    "group_sums = grouped[\"true_label\"].sum()\n",
    "\n",
    "# Count how many groups have at least one true positive (sum > 0)\n",
    "groups_with_positives = (group_sums > 0).sum()\n",
    "print(\n",
    "    f\"Number of encounter groups with at least one true positive: {groups_with_positives}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a770f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 21356\n",
      "['encounter_id', 'event_timestamp', 'culture_event', 'suspected_infection', 'true_label', 'prediction_proba_1', 'prediction_proba_2']\n",
      "Number of true labels (1): 789\n",
      "  encounter_id  event_timestamp  culture_event  suspected_infection  \\\n",
      "0     0666505c              2.0            1.0                  1.0   \n",
      "1     0666505c              2.0            1.0                  1.0   \n",
      "2     0666505c              3.0            1.0                  1.0   \n",
      "3     0666505c              3.0            1.0                  1.0   \n",
      "4     0666505c              4.0            1.0                  1.0   \n",
      "\n",
      "   true_label  prediction_proba_1  prediction_proba_2  \n",
      "0           1            0.185334            0.531925  \n",
      "1           1            0.185334            0.531925  \n",
      "2           1            0.134316            0.360046  \n",
      "3           1            0.134316            0.360046  \n",
      "4           1            0.118005            0.167655  \n",
      "Number of unique encounter groups: 100\n",
      "Number of encounter groups with at least one true positive: 50\n"
     ]
    }
   ],
   "source": [
    "sample_data_path = os.path.join(\"data\", \"anonymised_sample.parquet\")\n",
    "# count rows\n",
    "table = pq.read_table(sample_data_path)\n",
    "print(f\"Number of rows: {table.num_rows}\")\n",
    "# print column names\n",
    "print(table.column_names)\n",
    "# Convert the 'true_label' column to a pandas Series\n",
    "true_label_series = table[\"true_label\"].to_pandas()\n",
    "\n",
    "# Count the number of true labels (1)\n",
    "true_count = (true_label_series == 1).sum()\n",
    "\n",
    "print(f\"Number of true labels (1): {true_count}\")\n",
    "\n",
    "# Convert the table to a pandas DataFrame for easier grouping\n",
    "df = table.to_pandas()\n",
    "\n",
    "# show table\n",
    "print(df.head(5))\n",
    "\n",
    "\n",
    "grouped = df.groupby(\"encounter_id\")\n",
    "\n",
    "# Count the number of unique groups (encounters)\n",
    "num_groups = df[\"encounter_id\"].nunique()\n",
    "print(f\"Number of unique encounter groups: {num_groups}\")\n",
    "\n",
    "# Calculate the sum of 'true_label' for each group\n",
    "group_sums = grouped[\"true_label\"].sum()\n",
    "\n",
    "# Count how many groups have at least one true positive (sum > 0)\n",
    "groups_with_positives = (group_sums > 0).sum()\n",
    "print(\n",
    "    f\"Number of encounter groups with at least one true positive: {groups_with_positives}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea57d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded data with assigned names (Model 1):\n",
      "encounter_id: string\n",
      "event_timestamp: double\n",
      "culture_event: double\n",
      "suspected_infection: double\n",
      "true_label: int64\n",
      "prediction_proba_1: float\n",
      "prediction_proba_2: float\n",
      "-- schema metadata --\n",
      "pysalient.io.y_proba_col: 'prediction_proba_1'\n",
      "pysalient.io.y_label_col: 'true_label'\n",
      "pysalient.io.timeseries_col: 'event_timestamp'\n",
      "pysalient.io.aggregation_cols: '[]'\n",
      "\n",
      "Number of rows: 21356\n",
      "\n",
      "First 5 rows (with added 'task' and 'model' columns):\n",
      "  encounter_id  event_timestamp  culture_event  suspected_infection  \\\n",
      "0     0666505c              2.0            1.0                  1.0   \n",
      "1     0666505c              2.0            1.0                  1.0   \n",
      "2     0666505c              3.0            1.0                  1.0   \n",
      "3     0666505c              3.0            1.0                  1.0   \n",
      "4     0666505c              4.0            1.0                  1.0   \n",
      "\n",
      "   true_label  prediction_proba_1  prediction_proba_2  \n",
      "0           1            0.185334            0.531925  \n",
      "1           1            0.185334            0.531925  \n",
      "2           1            0.134316            0.360046  \n",
      "3           1            0.134316            0.360046  \n",
      "4           1            0.118005            0.167655  \n",
      "\n",
      "Successfully loaded data with assigned names (Model 1):\n",
      "encounter_id: string\n",
      "event_timestamp: double\n",
      "culture_event: double\n",
      "suspected_infection: double\n",
      "true_label: int64\n",
      "prediction_proba_1: float\n",
      "prediction_proba_2: float\n",
      "-- schema metadata --\n",
      "pysalient.io.y_proba_col: 'prediction_proba_2'\n",
      "pysalient.io.y_label_col: 'true_label'\n",
      "pysalient.io.timeseries_col: 'event_timestamp'\n",
      "pysalient.io.aggregation_cols: '[]'\n",
      "\n",
      "Number of rows: 21356\n",
      "\n",
      "First 5 rows (with added 'task' and 'model' columns):\n",
      "  encounter_id  event_timestamp  culture_event  suspected_infection  \\\n",
      "0     0666505c              2.0            1.0                  1.0   \n",
      "1     0666505c              2.0            1.0                  1.0   \n",
      "2     0666505c              3.0            1.0                  1.0   \n",
      "3     0666505c              3.0            1.0                  1.0   \n",
      "4     0666505c              4.0            1.0                  1.0   \n",
      "\n",
      "   true_label  prediction_proba_1  prediction_proba_2  \n",
      "0           1            0.185334            0.531925  \n",
      "1           1            0.185334            0.531925  \n",
      "2           1            0.134316            0.360046  \n",
      "3           1            0.134316            0.360046  \n",
      "4           1            0.118005            0.167655  \n"
     ]
    }
   ],
   "source": [
    "# Define the path relative to the project root\n",
    "# Assuming the notebook is run from the project root or examples/ directory\n",
    "sample_data_path = os.path.join(\"data\", \"anonymised_sample.parquet\")\n",
    "\n",
    "assigned_table_events = None\n",
    "\n",
    "if os.path.exists(sample_data_path):\n",
    "    # Use the actual column names identified during inspection directly\n",
    "    # Ensure these names actually exist based on the printout above!\n",
    "    model_1_evaluation = io.load_evaluation_data(\n",
    "        source=sample_data_path,\n",
    "        y_proba_col=\"prediction_proba_1\",\n",
    "        y_label_col=\"true_label\",\n",
    "        aggregation_cols=None,\n",
    "        timeseries_col=\"event_timestamp\",\n",
    "        # We don't provide task_col or model_col from the source\n",
    "        # assign_task_name=\"AKI\",  # Assign this name to the new 'task' column\n",
    "        # assign_model_name=\"LogRegress\",  # Assign this name to the new 'model' column\n",
    "    )\n",
    "\n",
    "    print(\"\\nSuccessfully loaded data with assigned names (Model 1):\")\n",
    "    print(model_1_evaluation.schema)\n",
    "    print(f\"\\nNumber of rows: {model_1_evaluation.num_rows}\")\n",
    "\n",
    "    # Display first few rows to verify new columns\n",
    "    print(\"\\nFirst 5 rows (with added 'task' and 'model' columns):\")\n",
    "    print(model_1_evaluation.slice(0, 5).to_pandas())\n",
    "    model_2_evaluation = io.load_evaluation_data(\n",
    "        source=sample_data_path,\n",
    "        y_proba_col=\"prediction_proba_2\",\n",
    "        y_label_col=\"true_label\",\n",
    "        aggregation_cols=None,\n",
    "        timeseries_col=\"event_timestamp\",\n",
    "        # We don't provide task_col or model_col from the source\n",
    "        # assign_task_name=\"AKI\",  # Assign this name to the new 'task' column\n",
    "        # assign_model_name=\"LogRegress\",  # Assign this name to the new 'model' column\n",
    "    )\n",
    "    print(\"\\nSuccessfully loaded data with assigned names (Model 1):\")\n",
    "    print(model_2_evaluation.schema)\n",
    "    print(f\"\\nNumber of rows: {model_2_evaluation.num_rows}\")\n",
    "\n",
    "    # Display first few rows to verify new columns\n",
    "    print(\"\\nFirst 5 rows (with added 'task' and 'model' columns):\")\n",
    "    print(model_2_evaluation.slice(0, 5).to_pandas())\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Skipping data loading as file was not found: {sample_data_path}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d147ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/dev/pysalient/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "Metric func '_calculate_npv_boot': All bootstrap rounds failed calculation; cannot compute CI. This may indicate an issue with the metric calculation or the bootstrap sample characteristics.\n",
      "Metric func '_calculate_ppv_boot': All bootstrap rounds failed calculation; cannot compute CI. This may indicate an issue with the metric calculation or the bootstrap sample characteristics.\n",
      "Metric func '_calculate_ppv_boot': All bootstrap rounds failed calculation; cannot compute CI. This may indicate an issue with the metric calculation or the bootstrap sample characteristics.\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation parameters\n",
    "eval_modelid_1= \"LogRegress_01\"  # Use a generic ID as model wasn't assigned here\n",
    "eval_modelid_2= \"LightGBM_01\"  # Use a generic ID as model wasn't assigned here\n",
    "eval_filter = \"ExampleFilterDummy\"  # Describe the data subset\n",
    "eval_thresholds = (0.1, 0.9, 0.1)  # Range: 0.1, 0.2, ..., 0.9\n",
    "# eval_thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9] # Example: List of thresholds\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation_results_1 = evaluation(\n",
    "    data=model_1_evaluation,  # Use the table loaded with col_map\n",
    "    modelid=eval_modelid_1,\n",
    "    filter_desc=eval_filter,\n",
    "    thresholds=eval_thresholds,\n",
    "    decimal_places=3,  # Control rounding of output floats # check that -1 is no rounding.\n",
    "    calculate_au_ci=True,  # Enable AU CI calculation (uses bootstrap)\n",
    "    calculate_threshold_ci=True,\n",
    "    threshold_ci_method=\"bootstrap\",  # Method for threshold CIs (ignored if calculate_threshold_ci=False)\n",
    "    ci_alpha=0.05,  # 95% CI\n",
    "    bootstrap_seed=42,  # For reproducible CIs\n",
    "    bootstrap_rounds=1000,  # Fewer rounds for notebook speed\n",
    "    force_threshold_zero=True,\n",
    "    verbosity=1,\n",
    ")\n",
    "evaluation_results_2 = evaluation(\n",
    "    data=model_2_evaluation,  # Use the table loaded with col_map\n",
    "    modelid=eval_modelid_2,\n",
    "    filter_desc=eval_filter,\n",
    "    thresholds=eval_thresholds,\n",
    "    decimal_places=3,  # Control rounding of output floats # check that -1 is no rounding.\n",
    "    calculate_au_ci=True,  # Enable AU CI calculation (uses bootstrap)\n",
    "    calculate_threshold_ci=True,\n",
    "    threshold_ci_method=\"bootstrap\",  # Method for threshold CIs (ignored if calculate_threshold_ci=False)\n",
    "    ci_alpha=0.05,  # 95% CI\n",
    "    bootstrap_seed=42,  # For reproducible CIs\n",
    "    bootstrap_rounds=1000,  # Fewer rounds for notebook speed\n",
    "    force_threshold_zero=True,\n",
    "    verbosity=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d116a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare = compare_models(\n",
    "    evaluation_results=[evaluation_results_1,evaluation_results_2],\n",
    "    model_labels=['lg','lgm'],\n",
    "    include_metrics=['AUROC','AUPRC']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "styled_results = vis.format_evaluation_table(\n",
    "    evaluation_results_1, decimal_places=3, ci_column=False\n",
    ")\n",
    "display(styled_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f1534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "styled_results = vis.format_evaluation_table(\n",
    "    evaluation_results_2, decimal_places=3, ci_column=False\n",
    ")\n",
    "display(styled_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7869ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_results = vis.format_evaluation_table(\n",
    "    model_compare, decimal_places=3, ci_column=False\n",
    ")\n",
    "display(styled_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a06191",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
