{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from pysalient import io as io\n",
    "from pysalient import visualisation as vis\n",
    "from pysalient.evaluation import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a770f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_path = os.path.join(\"data\", \"anonymised_sample.parquet\")\n",
    "# count rows\n",
    "table = pq.read_table(sample_data_path)\n",
    "print(f\"Number of rows: {table.num_rows}\")\n",
    "# print column names\n",
    "print(table.column_names)\n",
    "# Convert the 'true_label' column to a pandas Series\n",
    "true_label_series = table[\"true_label\"].to_pandas()\n",
    "\n",
    "# Count the number of true labels (1)\n",
    "true_count = (true_label_series == 1).sum()\n",
    "\n",
    "print(f\"Number of true labels (1): {true_count}\")\n",
    "\n",
    "# Convert the table to a pandas DataFrame for easier grouping\n",
    "df = table.to_pandas()\n",
    "\n",
    "# show table\n",
    "print(df.head(5))\n",
    "\n",
    "\n",
    "grouped = df.groupby(\"encounter_id\")\n",
    "\n",
    "# Count the number of unique groups (encounters)\n",
    "num_groups = df[\"encounter_id\"].nunique()\n",
    "print(f\"Number of unique encounter groups: {num_groups}\")\n",
    "\n",
    "# Calculate the sum of 'true_label' for each group\n",
    "group_sums = grouped[\"true_label\"].sum()\n",
    "\n",
    "# Count how many groups have at least one true positive (sum > 0)\n",
    "groups_with_positives = (group_sums > 0).sum()\n",
    "print(\n",
    "    f\"Number of encounter groups with at least one true positive: {groups_with_positives}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path relative to the project root\n",
    "# Assuming the notebook is run from the project root or examples/ directory\n",
    "sample_data_path = os.path.join(\"data\", \"anonymised_sample.parquet\")\n",
    "\n",
    "assigned_table_events = None\n",
    "\n",
    "if os.path.exists(sample_data_path):\n",
    "    # Use the actual column names identified during inspection directly\n",
    "    # Ensure these names actually exist based on the printout above!\n",
    "    assigned_table_events = io.load_evaluation_data(\n",
    "        source=sample_data_path,\n",
    "        y_proba_col=\"prediction_proba_1\",\n",
    "        y_label_col=\"true_label\",\n",
    "        aggregation_cols=\"encounter_id\",\n",
    "        timeseries_col=\"event_timestamp\",\n",
    "        perform_aggregation=False,\n",
    "\n",
    "        # We don't provide task_col or model_col from the source\n",
    "        # assign_task_name=\"AKI\",  # Assign this name to the new 'task' column\n",
    "        # assign_model_name=\"LogRegress\",  # Assign this name to the new 'model' column\n",
    "    )\n",
    "\n",
    "    print(\"\\nSuccessfully loaded data with assigned names (Example 1):\")\n",
    "    print(assigned_table_events.schema)\n",
    "    print(f\"\\nNumber of rows: {assigned_table_events.num_rows}\")\n",
    "\n",
    "    # Display first few rows to verify new columns\n",
    "    print(\"\\nFirst 5 rows (with added 'task' and 'model' columns):\")\n",
    "    print(assigned_table_events.slice(0, 5).to_pandas())\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Skipping data loading (Example 1) as file was not found: {sample_data_path}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92451b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation parameters\n",
    "eval_modelid = \"LogRegress_01\"  # Use a generic ID as model wasn't assigned here\n",
    "eval_filter = \"ExampleFilterDummy\"  # Describe the data subset\n",
    "eval_thresholds = (0.01, 0.1, 0.01)  # Range: 0.1, 0.2, ..., 0.9\n",
    "# eval_thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9] # Example: List of thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d147ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "evaluation_results = evaluation(\n",
    "    data=assigned_table_events,  # Use the table loaded with col_map\n",
    "    modelid=eval_modelid,\n",
    "    filter_desc=eval_filter,\n",
    "    thresholds=eval_thresholds,\n",
    "    decimal_places=3,  # Control rounding of output floats # check that -1 is no rounding.\n",
    "    calculate_au_ci=True,  # Enable AU CI calculation (uses bootstrap)\n",
    "    calculate_threshold_ci=True,\n",
    "    threshold_ci_method=\"bootstrap\",  # Method for threshold CIs (ignored if calculate_threshold_ci=False)\n",
    "    ci_alpha=0.05,  # 95% CI\n",
    "    bootstrap_seed=42,  # For reproducible CIs\n",
    "    bootstrap_rounds=1000,  # Fewer rounds for notebook speed\n",
    "    force_threshold_zero=True,\n",
    "    verbosity=1,\n",
    "    time_to_event_cols={'bc': 'culture_event', 'sofa': 'suspected_infection'},\n",
    "    aggregation_func='median',\n",
    "    time_to_event_fillna=0,\n",
    "    time_unit='hour',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f1534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "styled_results = vis.format_evaluation_table(\n",
    "    evaluation_results, decimal_places=3, ci_column=False\n",
    ")\n",
    "display(styled_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9851bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_path = os.path.join(\"data\", \"anonymised_sample.parquet\")\n",
    "\n",
    "assigned_table_agg = None\n",
    "\n",
    "if os.path.exists(sample_data_path):\n",
    "    # Use the actual column names identified during inspection directly\n",
    "    # Ensure these names actually exist based on the printout above!\n",
    "    assigned_table_agg = io.load_evaluation_data(\n",
    "        source=sample_data_path,\n",
    "        y_proba_col=\"prediction_proba_1\",\n",
    "        y_label_col=\"true_label\",\n",
    "        aggregation_cols=\"encounter_id\",\n",
    "        proba_agg_func=\"max\",\n",
    "        label_agg_func=\"max\",\n",
    "        timeseries_col=\"event_timestamp\",\n",
    "        perform_aggregation=True,  # Explicitly enable aggregation\n",
    "        # We don't provide task_col or model_col from the source\n",
    "        # assign_task_name=\"AKI\",  # Assign this name to the new 'task' column\n",
    "        # assign_model_name=\"LogRegress\",  # Assign this name to the new 'model' column\n",
    "    )\n",
    "\n",
    "    print(\"\\nSuccessfully loaded data with assigned names (Example 1):\")\n",
    "    print(assigned_table_agg.schema)\n",
    "    print(f\"\\nNumber of rows: {assigned_table_agg.num_rows}\")\n",
    "\n",
    "    # Display first few rows to verify new columns\n",
    "    print(\"\\nFirst 5 rows (with added 'task' and 'model' columns):\")\n",
    "    print(assigned_table_agg.slice(0, 5).to_pandas())\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Skipping data loading (Example 1) as file was not found: {sample_data_path}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_agg = evaluation(\n",
    "    data=assigned_table_agg,  # Use the table loaded with col_map\n",
    "    modelid=eval_modelid,\n",
    "    filter_desc=eval_filter,\n",
    "    thresholds=eval_thresholds,\n",
    "    decimal_places=3,  # Control rounding of output floats # check that -1 is no rounding.\n",
    "    calculate_au_ci=True,  # Enable AU CI calculation (uses bootstrap)\n",
    "    calculate_threshold_ci=True,\n",
    "    threshold_ci_method=\"bootstrap\",  # Method for threshold CIs (ignored if calculate_threshold_ci=False)\n",
    "    ci_alpha=0.05,  # 95% CI\n",
    "    bootstrap_seed=42,  # For reproducible CIs\n",
    "    bootstrap_rounds=1000,  # Fewer rounds for notebook speed\n",
    "    force_threshold_zero=True,\n",
    "    verbosity=1,\n",
    "    time_to_event_cols={'bc': 'culture_event', 'sofa': 'suspected_infection'},\n",
    "    aggregation_func='median',\n",
    "    time_to_event_fillna=0,\n",
    "    time_unit='hour',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ca492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "styled_results = vis.format_evaluation_table(\n",
    "    evaluation_results_agg, decimal_places=3, ci_column=False\n",
    ")\n",
    "display(styled_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a06191",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
