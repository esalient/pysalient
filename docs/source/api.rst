API Reference
=============

This section provides detailed API references for the pysSALIENT framework modules.

IO Module
---------

.. automodule:: pysalient.io
   :members:

Functions for exporting results:

.. autofunction:: pysalient.io.export_evaluation_results

.. autofunction:: pysalient.io.export_formatted_results

Evaluation Module
-----------------

.. automodule:: pysalient.evaluation
   :members:


Visualisation and Display Helpers
---------------------------------

This module provides helper functions for visualizing and displaying results generated by other pysalient modules, particularly the evaluation results table.

.. automodule:: pysalient.visualisation
   :members:
   :undoc-members:
   :show-inheritance:

Usage Example
^^^^^^^^^^^^^

The primary function currently is `format_evaluation_table`, which helps display the float values in the evaluation results table with consistent formatting in environments like Jupyter notebooks.

.. code-block:: python

   import pysalient.evaluation as eval
   import pysalient.visualisation as vis
   import pysalient.io as io # Assuming io is used to load data

   # --- Load your data ---
   # Example using sample data and assigned names
   sample_data_path = 'data/anonymised_sample.parquet' # Adjust path as needed
   col_map = {
       'y_proba': 'prediction_probability',
       'y_label': 'true_label',
       'agg': 'encounter_id',
       'time': 'event_timestamp',
   }
   assigned_table = io.load_evaluation_data(
       source=sample_data_path,
       y_proba_col=col_map['y_proba'],
       y_label_col=col_map['y_label'],
       aggregation_cols=col_map['agg'],
       timeseries_col=col_map['time'],
       assign_task_name="AKI",
       assign_model_name="LogRegress",
       perform_aggregation=True,  # Enable aggregation by encounter
       label_agg_func="max"  # Use max aggregation for labels (any positive makes group positive)
   )

   # --- Run evaluation ---
   # Note: 'assigned_table' was loaded with 'timeseries_col' set to 'event_timestamp'.
   # The 'evaluation' function will use this along with the new event timing parameters.
   results_table = eval.evaluation(
       data=assigned_table,
       modelid="BaselineLogisticRegression",
       filter_desc="placeholder_filter",
       thresholds=(0.1, 0.9, 0.1),
       timeseries_col=col_map['time'], # Explicitly pass, matches what load_evaluation_data uses
       time_unit="hours", # Global unit for all time-to-event calculations.
                          # Mandatory if 'timeseries_col' is provided and event timing is performed.
       event_columns_for_timing=['some_binary_event_column'], # Example event column
       # event_column_time_units has been removed.
       decimal_places=3 # Evaluation rounding (optional)
   )

   # The 'results_table' will now contain dynamic columns such as
   # 'time_to_first_some_binary_event_column_value' for each event specified
   # in 'event_columns_for_timing', and a single shared 'time_to_event_units' column
   # populated with the global 'time_unit' (e.g., "hours").
   # Legacy 'time_to_first_alert_value' may also be present if applicable.

   # --- Format for display ---
   # Use the helper to format float columns (e.g., to 3 decimal places)
   styled_table = vis.format_evaluation_table(results_table, decimal_places=3)

   # Display in Jupyter/IPython
   # display(styled_table) # Uncomment in a notebook environment

   # If not in Jupyter, you might render to HTML
   # html_output = styled_table.render()
   # print(html_output)
